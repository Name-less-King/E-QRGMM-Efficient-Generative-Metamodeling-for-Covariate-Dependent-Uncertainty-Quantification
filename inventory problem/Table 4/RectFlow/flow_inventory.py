from utils import *  # provides get_args, train_all, generate, etc.
import os

import numpy as np
import torch
from tqdm import trange

DEVICE = "cpu"


class Dataset:
    def __init__(self, name=None, x=None, y=None):
        """
        Dataset class to generate test instances.
        If (x, y) are provided, use them directly; otherwise read from CSV according to name.
        """
        # ------- Raw data (before standardization) -------
        if x is not None and y is not None:
            x_arr = np.asarray(x, dtype=np.float32)
            y_arr = np.asarray(y, dtype=np.float32).ravel()
        else:
            if name is None:
                raise ValueError("You must provide either name or (x, y).")
            x_arr, y_arr = self.read_data(name)
            x_arr = np.asarray(x_arr, dtype=np.float32)
            y_arr = np.asarray(y_arr, dtype=np.float32).ravel()

        self.x = x_arr
        self.y = y_arr

        # ------- Standardize X -------
        self.x_mean = self.x.mean(axis=0)
        self.x_std = self.x.std(axis=0)
        self.x_std[self.x_std == 0.0] = 1.0
        x_norm = (self.x - self.x_mean) / self.x_std

        # ------- Standardize Y -------
        self.y_mean = float(self.y.mean())
        self.y_std = float(self.y.std())
        if self.y_std == 0.0:
            self.y_std = 1.0
        y_norm = (self.y - self.y_mean) / self.y_std

        # ------- Feed standardized x / y into the model -------
        self.x_train = torch.as_tensor(x_norm, dtype=torch.float32)
        self.y_train = torch.as_tensor(y_norm, dtype=torch.float32).view(-1, 1)
        self.device = DEVICE  # keep consistent with usage in utils/train_all

    def bootstrap(self):
        """Bootstrap resampling, returning a new Dataset instance."""
        idx = np.random.choice(len(self.x), size=len(self.x), replace=True)
        x_boot = self.x[idx]
        y_boot = self.y[idx]
        # Do not use name here; directly use the provided x, y (original scale),
        # and standardize again internally.
        return Dataset(x=x_boot, y=y_boot)

    def read_data(self, name: int):
        """
        Read IM_train_data/IM_train_data_{name}.csv generated by data_generation.py

        Each row: [s, S, mu, cost]
        x = (s, S, mu), y = cost
        """
        import pandas as pd

        csv_path = os.path.join("IM_train_data", f"IM_train_data_{name}.csv")
        if not os.path.exists(csv_path):
            raise FileNotFoundError(f"Data file not found: {csv_path}")

        df = pd.read_csv(csv_path, header=None)

        x_train = df.iloc[:, 0:3].values  # s, S, mu
        y_train = df.iloc[:, 3].values    # cost
        return x_train, y_train


def modify_args(args, data_set=None, data_type=None):
    if data_set is None and data_type is None:
        instance = str(args["data_type"]) + "_" + str(args["data_set"])
    else:
        instance = str(data_type) + "_" + str(data_set)
        args["data_set"] = data_set
        args["data_type"] = data_type

    args["instance"] = instance
    os.makedirs(f"models/{instance}", exist_ok=True)
    os.makedirs(f"results/{instance}", exist_ok=True)
    return args


def boot_once(x, y, k, args, name):
    """
    One bootstrap run:
    1. Perform bootstrap on (x, y) to get data_boot (which internally standardizes X and Y).
    2. Train the rectified flow model.
    3. For a single test point x_test_raw, generate k samples
       (first standardize using data_boot's X statistics).
    4. Transform generated y back to the original scale and output
       mean / quantile / probability above a given threshold.
    """
    # Perform bootstrap on the original scale
    idx = np.random.choice(len(x), size=len(x), replace=True)
    x_boot = x[idx]
    y_boot = y[idx]
    data_boot = Dataset(x=x_boot, y=y_boot)

    # Standardize the test point using this bootstrap dataset's X mean/std
    x_test_norm = (x_test_raw - data_boot.x_mean) / data_boot.x_std
    x_test_tensor = torch.as_tensor(x_test_norm, dtype=torch.float32).to(DEVICE).view(1, -1)

    # Train rectified flow
    train_all(data_boot, args, "rectified")

    # Generate samples (in standardized space), then unstandardize y back to original scale
    y_pred = generate(x_test_tensor, args, "rectified", sample_num=k).cpu().numpy()
    y_pred = y_pred * data_boot.y_std + data_boot.y_mean
    y_pred_flat = y_pred.flatten()

    mean = float(np.mean(y_pred_flat))
    quantile = float(np.percentile(y_pred_flat, 80))
    prob = float(np.sum(y_pred_flat > quantile_gt) / len(y_pred_flat))

    return [mean, quantile, prob]


# Test point: a given (s, S, mu)
s0 = 320
S0 = 420
mu0 = 330
# Test point in the original scale (the inventory policy we care about)
x_test_raw = np.array([s0, S0, mu0], dtype=np.float32)

# Model and experiment settings
args = get_args("inventory")  
args = modify_args(args)

# The following three are true mean/quantile/probability,
# which should be estimated via large-scale simulation;
# in actual use, please replace them with your own ground truth.
mean_gt = 357.86
quantile_gt = 366.72
prob_gt = 0.20

mean_cover_flag = 0
quantile_cover_flag = 0
prob_cover_flag = 0

m_length = []
q_length = []
p_length = []


runi = 100         # number of repetitions
num_samples = 100  # number of bootstrap samples per repetition
k = 100000         # number of samples generated each time

with trange(runi, dynamic_ncols=False) as pbar:
    for iter in pbar:
        name = iter + 1  
        data = Dataset(name)

        boot_list = []

        for _ in range(num_samples):
            # Here we pass data.x / data.y at the original scale
            result = boot_once(data.x, data.y, k, args, name)
            boot_list.append(result)

        boot_list_array = np.array(boot_list)
        mean_list = boot_list_array[:, 0]
        quantile_list = boot_list_array[:, 1]
        prob_list = boot_list_array[:, 2]

        # Coverage of mean interval
        mean_q5 = np.percentile(mean_list, 5)
        mean_q95 = np.percentile(mean_list, 95)
        if mean_q5 < mean_gt < mean_q95:
            mean_cover_flag += 1
        mean_coverage = mean_cover_flag / (iter + 1)

        # Coverage of 80% quantile interval
        quantile_q5 = np.percentile(quantile_list, 5)
        quantile_q95 = np.percentile(quantile_list, 95)
        if quantile_q5 < quantile_gt < quantile_q95:
            quantile_cover_flag += 1
        quantile_coverage = quantile_cover_flag / (iter + 1)

        # Coverage of probability interval for exceeding quantile_gt
        prob_q5 = np.percentile(prob_list, 5)
        prob_q95 = np.percentile(prob_list, 95)
        if prob_q5 < prob_gt < prob_q95:
            prob_cover_flag += 1
        prob_coverage = prob_cover_flag / (iter + 1)

        m_length.append(mean_q95 - mean_q5)
        q_length.append(quantile_q95 - quantile_q5)
        p_length.append(prob_q95 - prob_q5)

        pbar.set_postfix(
            {
                "\n m coverage": mean_coverage,
                "m lb": f"{mean_q5:.4f}",
                "m ub": f"{mean_q95:.4f}",
                "\n q coverage": quantile_coverage,
                "q lb": f"{quantile_q5:.4f}",
                "q ub": f"{quantile_q95:.4f}",
                "\n p coverage": prob_coverage,
                "p lb": f"{prob_q5:.4f}",
                "p ub": f"{prob_q95:.4f}",
            }
        )

print("mean length: ", np.mean(m_length), np.std(m_length) / np.sqrt(runi))
print("quantile length: ", np.mean(q_length), np.std(q_length) / np.sqrt(runi))
print("prob length: ", np.mean(p_length), np.std(p_length) / np.sqrt(runi))

